[["index.html", "JASP Verificiation Project Chapter 1 Introduction 1.1 The Project 1.2 Bibliography", " JASP Verificiation Project info@jasp-stats.org 2022-03-04 Chapter 1 Introduction 1.1 The Project This document contains the verification of the results computed in JASP. For this purpose, the same statistical tests have been performed on the same datasets, using the most popular statistical software packages and hand calculations. The comparison of the output is available for each test in the following chapters. The software packages that have been used for comparison are: Software Version JASP 0.13.1 SPSS 26.0.0 SAS 3.8 (Basic Edition) Minitab 19.2020.1 R 4.0.2 All code used for computation is available in this document. The datasets used can be found in the Github repository of this project: https://github.com/jasp-stats/jasp-verification-project 1.2 Bibliography Field, A. (2018). Discovering statistics using IBM SPSS statistics. Los Angeles, CA: SAGE. Freeman, E., Heathcote, A., Chalmers, K., &amp; Hockley, W. (2010). Item effects in recognition memory for words. Journal of Memory and Language, 62(1), 1-18. Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. Kerlinger, F. N. (1969). Foundations of behavioral research. New York, US: Holt, Rinehart and Winston, Inc. "],["descriptives.html", "Chapter 2 Descriptives 2.1 Descriptives", " Chapter 2 Descriptives 2.1 Descriptives An example from Kerlinger (1969, pp. 93-95): Table 2.1: Data for Descriptives Data 1 2 3 4 5 2.1.1 Results Overview Table 2.2: Result Overview Descriptives By Hand JASP SPSS SAS Minitab R Mean 3.0 3.000 3.000 3.000 3.000 3.000 Variance 2.5 2.500 2.500 2.500 2.500 2.500 Median NA 3.000 3.000 3.000 3.000 3.000 Standard Deviation NA 1.580 1.580 1.580 1.580 1.580 SE(Mean) NA 0.707 0.707 0.707 0.707 0.707 2.1.2 By Hand Calculations by hand can be found in Kerlinger (1969, pp. 93-95). Result: Mean = 3 Variance = 2.5 Note: Kerlinger calculated the population variance, however as all statistical software computes the sample variance, the formula was adapted accordingly to be divided by N-1. 2.1.3 JASP Figure 2.1: JASP Output for Descriptives 2.1.4 SPSS DATASET ACTIVATE DataSet1. DESCRIPTIVES VARIABLES=Data /STATISTICS=MEAN STDDEV VARIANCE MIN MAX SEMEAN. Figure 2.2: SPSS Output for Descriptives 2.1.5 SAS PROC MEANS DATA=work.Desc Mean STDDEV Median STDERR Var; VAR Data; RUN; Figure 2.3: SAS Output for Descriptives 2.1.6 Minitab Figure 2.4: Minitab Output for Descriptives 2.1.7 R mean(desc.data2$Data) ## [1] 3 sd(desc.data2$Data) ## [1] 1.581139 var(desc.data2$Data) ## [1] 2.5 median(desc.data2$Data) ## [1] 3 se &lt;- function(x) sqrt(var(x)/length(x)) se(desc.data2$Data) ## [1] 0.7071068 2.1.8 Remarks All differences in results between the software and hand calculation are due to rounding. 2.1.9 References Kerlinger, F. N. (1969). Foundations of behavioral research. New York, US: Holt, Rinehart and Winston, Inc. "],["t-tests.html", "Chapter 3 T-Tests 3.1 Independent Samples T-Test 3.2 Mann-Whitney Test 3.3 Paired Samples T-Test 3.4 Wilcoxon Test 3.5 One Sample T-Test", " Chapter 3 T-Tests 3.1 Independent Samples T-Test An example from Hays (1974, pp. 404-407): “An experimenter working in the area of motivational factors in perception was interested in the effects of deprivation upon the perceived size of objects. Among the studies carried out was one done with orphans, who were compared with nonorphaned children on the basis of the judged size of parental figures viewed at a distance. […]Now two independent randomly selected groups were used. Sample 1 was a group of orphaned children without foster parents. Sample 2 was a group of children having a normal family with both parents. Both population of children sampled showed the same age level, sex distribution, educational level, and so forth. The question asked by the experimenter was ‘Do deprived children tend to judge the parental figures relatively larger than do the nondeprived?’ In terms of a null and alternative hypothesis, H0: \\(\\mu\\)1 - \\(\\mu\\)2 \\(\\le 0\\) H1: \\(\\mu\\)1 - \\(\\mu\\)2 \\(&gt; 0\\). The \\(\\alpha\\) level for significance decided upon was .05. The actual results were Sample 1: M1 = 1.8 S1 = .7 N1 = 125 Sample 2: M2 = 1.6 S2 = .9 N2 = 150” Note: A data set with these properties has been simulated using R. 3.1.1 Results Overview Table 3.1: Result Overview Independent Samples T-Test By Hand JASP SPSS SAS Minitab R t (Welch) 2.11 2.0717 2.072 2.07 2.07 2.0717 t (Student) NA 2.0257 2.026 2.03 2.03 2.0257 3.1.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 404-407. Result: t = 2.11 Significant (two-tailed test) for \\(\\alpha\\) = .05 or less Note: Hays calculated only the Welch T-test. 3.1.3 JASP Figure 3.1: JASP Output for Independent Samples T-Test 3.1.4 SPSS DATASET ACTIVATE DataSet1. T-TEST GROUPS=groups(1 2) /MISSING=ANALYSIS /VARIABLES=samples /CRITERIA=CI(.95). Figure 3.2: SPSS Output for Independent Samples T-Test 3.1.5 SAS proc ttest data=istt sides=2 alpha=0.05 h0=0; title &quot;Two sample t-test&quot;; class Group; var Score; run; Figure 3.3: SAS Output for Independent Samples T-Test 3.1.6 Minitab Figure 3.4: Minitab Output for Welch Independent Samples T-Test Figure 3.5: Minitab Output for Student Independent Samples T-Test 3.1.7 R t.test(subset(istt.data, Group == 1)$Score, subset(istt.data, Group == 2)$Score) ## ## Welch Two Sample t-test ## ## data: subset(istt.data, Group == 1)$Score and subset(istt.data, Group == 2)$Score ## t = 2.0717, df = 271.76, p-value = 0.03924 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.009938365 0.390061635 ## sample estimates: ## mean of x mean of y ## 1.8 1.6 t.test(subset(istt.data, Group == 1)$Score, subset(istt.data, Group == 2)$Score, var.equal = T) ## ## Two Sample t-test ## ## data: subset(istt.data, Group == 1)$Score and subset(istt.data, Group == 2)$Score ## t = 2.0257, df = 273, p-value = 0.04377 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.005624928 0.394375072 ## sample estimates: ## mean of x mean of y ## 1.8 1.6 3.1.8 Remarks All differences in results between the software and hand calculation are due to rounding. 3.1.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 3.2 Mann-Whitney Test An example from Hays (1974, pp. 778-780): “As an example, consider the following data:” Table 3.2: Data for Mann-Whitney Test Score Group 8 A 3 A 4 A 6 A 1 B 7 B 9 B 10 B 12 B 3.2.1 Results Overview Table 3.3: Result Overview Mann-Whitney Test By Hand JASP SPSS SAS Minitab R U’ 5 5 5 5 5 5 3.2.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 778-780. Result: U’ = 5 U = 15 Not significant for \\(\\alpha\\) = .05 or less. 3.2.3 JASP Figure 3.6: JASP Output for Mann-Whitney Test Note: W corresponds to U’ in the hand calculation. 3.2.4 SPSS DATASET ACTIVATE DataSet1. *Nonparametric Tests: Independent Samples. NPTESTS /INDEPENDENT TEST (Score) GROUP (Group) MANN_WHITNEY /MISSING SCOPE=ANALYSIS USERMISSING=EXCLUDE /CRITERIA ALPHA=0.05 CILEVEL=95. Figure 3.7: SPSS Output for Mann-Whitney Test Note: U = 15 corresponds to U’ = 5. 3.2.5 SAS PROC npar1way data=work.mwt wilcoxon; class Group; var Score; run; Figure 3.8: SAS Output for Mann-Whitney Test Note: U = 15 corresponds to U’ = 5. 3.2.6 Minitab Figure 3.9: Minitab Output for Mann-Whitney Test Note: W corresponds to U in the hand calculation and U = 15 corresponds to U’ = 5. 3.2.7 R wilcox.test(Score~Group, data = MWT.data2) ## ## Wilcoxon rank sum exact test ## ## data: Score by Group ## W = 5, p-value = 0.2857 ## alternative hypothesis: true location shift is not equal to 0 Note: W corresponds to U’ in the hand calculation. 3.2.8 Remarks All differences in results between the software and hand calculation are due to rounding. The output for the Mann-Whitney test is not clearly defined, leading to different conventions in different software. The R documentation explains it as follows: “The two most common definitions correspond to the sum of the ranks of the first sample with the minimum value subtracted or not: R subtracts and S-PLUS does not, giving a value which is larger by m(m+1)/2 for a first sample of size m. (It seems Wilcoxon’s original paper used the unadjusted sum of the ranks but subsequent tables subtracted the minimum.) R’s value can also be computed as the number of all pairs (x[i], y[j]) for which y[j] is not greater than x[i], the most common definition of the Mann-Whitney test.” 3.2.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 3.3 Paired Samples T-Test An example from Hays (1974, pp. 424-427): “Consider once again the question of scores on a test of dominance. The basic question has to do with the mean score for men as opposed to the mean score for women. In carrying out the experiment, the investigator decided to sample eight husband-wife-pairs at random. The members of each pair were given the test of dominance separately, and the data turned out as follows:” Table 3.4: Data for Paired Sample T-Test Husband Wife 26 30 28 29 28 28 29 27 30 26 31 25 34 24 37 23 3.3.1 Results Overview Table 3.5: Result Overview Paired Samples T-Test By Hand JASP SPSS SAS Minitab R t 1.838 1.8381 1.838 1.84 1.84 1.8381 3.3.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 424-427. Result: t = 1.838 Not significant (two-tailed test) for \\(\\alpha\\) = .05 or less. 3.3.3 JASP Figure 3.10: JASP Output for Paired Samples T-Test 3.3.4 SPSS DATASET ACTIVATE DataSet1. T-TEST PAIRS=Husband WITH Wife (PAIRED) /CRITERIA=CI(.9500) /MISSING=ANALYSIS. Figure 3.11: SPSS Output for Paired Samples T-Test 3.3.5 SAS proc ttest data=pstt sides=2 alpha=0.05 h0=0; title &quot;Paired sample t-test&quot;; paired Husband * Wife; run; Figure 3.12: SAS Output for Paired Samples T-Test 3.3.6 Minitab Figure 3.13: Minitab Output for Paired Samples T-Test 3.3.7 R t.test(pstt.data$Husband, pstt.data$Wife, paired = T) ## ## Paired t-test ## ## data: pstt.data$Husband and pstt.data$Wife ## t = 1.8381, df = 7, p-value = 0.1086 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.109927 8.859927 ## sample estimates: ## mean of the differences ## 3.875 3.3.8 Remarks All differences in results between the software and hand calculation are due to rounding. 3.3.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 3.4 Wilcoxon Test An example from Hays (1974, pp. 780-781): “Suppose that in some experiment involving a single treatment and one control group, subjects were first matched pairwise, and then one member of each pair was assigned to the experimental group at random. In the experiment proper, each subject received some Y score.” Table 3.6: Data for Wilcoxon Test Treatment. Control 83 75 80 78 81 66 74 77 79 80 78 68 72 75 84 90 85 81 88 83 3.4.1 Results Overview Table 3.7: Result Overview Wilcoxon Test By Hand JASP SPSS SAS Minitab R W’ 15 15 15 NA 15 15 3.4.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 780-781. Result: T = 15 Not significant for \\(\\alpha\\) = .05 or less. 3.4.3 JASP Figure 3.14: JASP Output for Wilcoxon Test 3.4.4 SPSS DATASET ACTIVATE DataSet1. *Nonparametric Tests: Related Samples. NPTESTS /RELATED TEST(Treatment Control) WILCOXON /MISSING SCOPE=ANALYSIS USERMISSING=EXCLUDE /CRITERIA ALPHA=0.05 CILEVEL=95. Figure 3.15: SPSS Output for Wilcoxon Test 3.4.5 SAS Not available in SAS. 3.4.6 Minitab Figure 3.16: Minitab Output for Wilcoxon Test 3.4.7 R wilcox.test(Wilcoxon.data2$Control, Wilcoxon.data2$Treatment., paired = T) ## Warning in wilcox.test.default(Wilcoxon.data2$Control, ## Wilcoxon.data2$Treatment., : cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: Wilcoxon.data2$Control and Wilcoxon.data2$Treatment. ## V = 15, p-value = 0.221 ## alternative hypothesis: true location shift is not equal to 0 3.4.8 Remarks All differences in results between the software and hand calculation are due to rounding. 3.4.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 3.5 One Sample T-Test A dataset with the following properties has been generated using R: n = 100 M = 5 \\(\\sigma\\) = 1 The expected mean to perform the One Sample T-test against is: E(M) = 3 The alpha level was chosen to be: \\(\\alpha\\) = .05 3.5.1 Results Overview Table 3.8: Result Overview One Sample T-Test By Hand JASP SPSS SAS Minitab R t 20 20 20 20 20 20 3.5.2 By Hand t = \\(\\frac{M - E(M) }{\\sigma/\\sqrt{n}}\\) therefore: t = \\(\\frac{5-3}{1/\\sqrt{100}}\\) = \\(\\frac{2}{1/10}\\) = 20 A t-score of 20 is significant for a two tailed test at \\(\\alpha\\) = .05 or less. 3.5.3 JASP Figure 3.17: JASP Output for One Sample T-Test 3.5.4 SPSS DATASET ACTIVATE DataSet1. T-TEST /TESTVAL=3 /MISSING=ANALYSIS /VARIABLES=V1 /CRITERIA=CI(.95). Figure 3.18: SPSS Output for One Sample T-Test 3.5.5 SAS proc ttest data=work.OSTT sides=2 alpha=0.05 h0=3; var Data; run; Figure 3.19: SAS Output for One Sample T-Test 3.5.6 Minitab Figure 3.20: Minitab Output for One Sample T-Test 3.5.7 R t.test(ostt.data2$V1, mu = 3) ## ## One Sample t-test ## ## data: ostt.data2$V1 ## t = 20, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 3 ## 95 percent confidence interval: ## 4.801578 5.198422 ## sample estimates: ## mean of x ## 5 3.5.8 Remarks All differences in results between the software and hand calculation are due to rounding. "],["anova.html", "Chapter 4 ANOVA 4.1 One-Way Independent ANOVA 4.2 Factorial Independent ANOVA 4.3 Kruskal-Wallis ANOVA 4.4 One-Way Repeated-Measures ANOVA 4.5 Friedman Test 4.6 ANCOVA 4.7 MANOVA", " Chapter 4 ANOVA 4.1 One-Way Independent ANOVA An example from Hays (1974, pp. 476-478): “An experiment was carried out to study the effect of a small lesion introduced into a particular structure in a rat’s brain on his ability to perform in a discrimination problem. The particular structure studied is bilaterally symmetric. so that the lesion could be introduced into the structure on the right side of the brain, the left side, both sides, or neither side (a control group). Four groups of randomly selected rats were formed, and given the various treatments.[…] After a period of postoperative recovery, each rats was given the same series of discrimination problems. The dependent variable score was the average number of trials it took each rat to learn the task to some criterion level. The null hypothesis was that the four treatment populations of rats are identical in their average ability to learn this task: H0: \\(\\mu\\)1 = \\(\\mu\\)2 = \\(\\mu\\)3 = \\(\\mu\\)4 as against the hypothesis that treatment differences exist: H1: not H0. The alpha level chosen for the experiment was .05.” Table 4.1: Data for One-Way Independent ANOVA Score Group 20 1 18 1 26 1 19 1 26 1 24 1 26 1 24 2 22 2 25 2 25 2 20 2 21 2 34 2 18 2 32 2 23 2 22 2 20 3 22 3 30 3 27 3 22 3 24 3 28 3 21 3 23 3 25 3 18 3 30 3 32 3 27 4 35 4 18 4 24 4 28 4 32 4 16 4 18 4 25 4 4.1.1 Results Overview Table 4.2: Result Overview One-Way Independent ANOVA By Hand JASP SPSS SAS Minitab R F 0.307377 0.3082 0.308 0.31 0.31 0.308 4.1.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 476-478. Results: Table 4.3: Result from Hays (1974) Source SS df MS F Treatments (between groups) 22.6 3 7.5 0.307377 Error (within groups) 878.9 36 24.4 NA Totals 901.5 39 NA NA Conclusion: The null hypothesis cannot be rejected, because the critical F value was 2.84. Note: Hays reported F =\\(\\frac{7.5}{24.4}\\). For a better comparison we transformed it to a decimal number. 4.1.3 JASP Figure 4.1: JASP Output for One-Way Independent ANOVA 4.1.4 SPSS DATASET ACTIVATE DataSet1. ONEWAY Score BY Group /MISSING ANALYSIS. Figure 4.2: SPSS Output for One-Way Independent ANOVA 4.1.5 SAS proc ANOVA data=anova; title One-way ANOVA; class Group; model Score = Group; means Group /hovtest welch; run; Figure 4.3: SAS Output for One-Way Independent ANOVA 4.1.6 Minitab Figure 4.4: Minitab Output for One-Way Independent ANOVA 4.1.7 R ## Compute the analysis of variance results.anova &lt;- aov(anova.data$Score ~ anova.data$Group, data = anova.data) ## Summary of the analysis summary(results.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## anova.data$Group 3 22.6 7.524 0.308 0.819 ## Residuals 36 878.9 24.415 4.1.8 Remarks All differences in results between the software and hand calculation are due to rounding. 4.1.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 4.2 Factorial Independent ANOVA An example from Hays (1974, pp. 491-493, 508-512): “Just as before, the experimental game is under the control of the experimenter, so that each subject actually obtains the same score. After a fixed number of trials, during which the subject gets the preassigned score, he is asked to predict what his score will be on the next group of trials. Before he predicts, the subject is given ‘information’ about how this score compares with some norm group. In one experimental condition he is told that his performance is above average for the norm group, in the second condition he is told that his score is average, and in the third condition he is told that his score is below average for the norm group. Once again, there are three experimental treatments in terms of ‘standings’: ‘above average’, ‘average’, ‘below average’. […]One half of the subjects are told that they are being compared with college men, and the other half are told that they are being compared with professional athletes. Hence, there are two additional experimental treatments: ‘college norms’, and ‘professional athlete norms’. We wish to examine three null hypotheses: there is no effect of the standing given the subject[…] the actual norm group given the subjects has no effect […] the norm-group-standing combination has no unique effect […] The \\(\\alpha\\) level chosen for each of these three tests will be .05.” Table 4.4: Data for Factorial Independent ANOVA Norms Above Average Below College men 52 28 15 College men 48 35 14 College men 43 34 23 College men 50 32 21 College men 43 34 14 College men 44 27 20 College men 46 31 21 College men 46 27 16 College men 43 29 20 College men 49 25 14 Professional Athlete 38 43 23 Professional Athlete 42 34 25 Professional Athlete 42 33 18 Professional Athlete 35 42 26 Professional Athlete 33 41 18 Professional Athlete 38 37 26 Professional Athlete 39 37 20 Professional Athlete 34 40 19 Professional Athlete 33 36 22 Professional Athlete 34 35 17 N = 60 4.2.1 Results Overview Table 4.5: Result Overview Independent Factorial ANOVA By Hand JASP SPSS SAS Minitab R F (Norm groups) 0.35 0.3582 0.358 0.36 0.36 0.358 F (Standings) 209.80 209.6418 209.642 209.64 209.64 209.642 F (Interaction) 34.00 34.0075 34.007 34.01 34.01 34.007 4.2.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 508-512. Results: Table 4.6: Result from Hays (1974) Source SS df MS F Rows (norm groups) 4.2 1 4.20 0.35 Columns (standings) 4994.1 2 2497.05 209.80 Interaction 810.2 2 405.10 34.00 Error (within cells) 643.2 54 11.90 NA Conclusion: The null hypothesis cannot be rejected, for the main effect of norm groups. It can be rejected for the main effect of standings and the interaction effect. 4.2.3 JASP Figure 4.5: JASP Output for Independent Factorial ANOVA 4.2.4 SPSS DATASET ACTIVATE DataSet1. UNIANOVA Score BY Norms Standing /METHOD=SSTYPE(3) /INTERCEPT=EXCLUDE /CRITERIA=ALPHA(0.05) /DESIGN=Norms Standing Norms*Standing. Figure 4.6: SPSS Output for Independent Factorial ANOVA 4.2.5 SAS proc anova data=FIanova; class Norms Standing; model Score = Norms Standing Norms*Standing; run; Figure 4.7: SAS Output for Independent Factorial ANOVA 4.2.6 Minitab Figure 4.8: Minitab Output for Independent Factorial ANOVA 4.2.7 R FIanova.data2 &lt;- read.csv(&quot;Datasets/FIanova.csv&quot;, sep=&quot;,&quot;) ## Compute the analysis of variance results.anova &lt;- aov(FIanova.data2$Score ~ FIanova.data2$Standing * FIanova.data2$Norms, data = FIanova.data2) ## Summary of the analysis summary(results.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## FIanova.data2$Standing 2 4994 2497.1 209.642 &lt; 2e-16 ## FIanova.data2$Norms 1 4 4.3 0.358 0.552 ## FIanova.data2$Standing:FIanova.data2$Norms 2 810 405.1 34.007 2.76e-10 ## Residuals 54 643 11.9 ## ## FIanova.data2$Standing *** ## FIanova.data2$Norms ## FIanova.data2$Standing:FIanova.data2$Norms *** ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.2.8 Remarks All differences in results between the software and hand calculation are due to rounding. 4.2.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 4.3 Kruskal-Wallis ANOVA An example from Hays (1974, pp. 782-784): “For example, suppose that three groups of small children were given the task of learning to discriminate between pairs of stimuli. Each child was given a series of pairs of stimuli, in which each pair differed in a variety of ways. However, attached to the choice of one member of a pair was a reward, and within an experimental condition, the cue for the rewarded stimulus was always the same. On the other hand, the experimental treatments themselves differed in the relevant cue for discrimination: in treatment I, the cue was form, in treatment II, color, and in treatment III, size. Some 36 children of the same sex and age were chosen at random and assigned at random to the three groups, with 12 children per group. The dependent variable was the number of trials to a fixed criterion of learning. Suppose that the data turned out the be as shown[…]” Table 4.7: Data for Kruskal-Wallis ANOVA Treatment.I Treatment.II Treatment.III 6 31 13 11 7 32 12 9 31 20 11 30 24 16 28 21 19 29 18 17 25 15 11 26 14 22 26 10 23 27 8 27 26 14 26 19 4.3.1 Results Overview Table 4.8: Result Overview Kruskal-Wallis ANOVA By Hand JASP SPSS SAS Minitab R H’ 13.85 13.8444 13.844 13.8444 13.84 13.844 4.3.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 782-784. Result: H = 13.85 Significant for \\(\\alpha\\) = .01 or less. 4.3.3 JASP Figure 4.9: JASP Output for Kruskal-Wallis ANOVA 4.3.4 SPSS DATASET ACTIVATE DataSet1. *Nonparametric Tests: Independent Samples. NPTESTS /INDEPENDENT TEST (Score) GROUP (Treatment) KRUSKAL_WALLIS(COMPARE=NONE) /MISSING SCOPE=ANALYSIS USERMISSING=EXCLUDE /CRITERIA ALPHA=0.05 CILEVEL=95. Figure 4.10: SPSS Output for Kruskal-Wallis ANOVA 4.3.5 SAS PROC npar1way data=work.KWanova ; class Treatment; var Score; run; Figure 4.11: SAS Output for Kruskal-Wallis ANOVA 4.3.6 Minitab Figure 4.12: Minitab Output for Kruskal-Wallis ANOVA 4.3.7 R kruskal.test(Score~Treatment, data = KWA.data2) ## ## Kruskal-Wallis rank sum test ## ## data: Score by Treatment ## Kruskal-Wallis chi-squared = 13.844, df = 2, p-value = 0.0009857 4.3.8 Remarks All differences in results between the software and hand calculation are due to rounding. 4.3.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 4.4 One-Way Repeated-Measures ANOVA An example from Kerlinger (1969, pp. 242-248): “A principal of a school and the members of his staff decided to introduce a program of education in intergroup relations as an addition to the school’s curriculum. One of the problems that arose was in the use of motion pictures. Films were shown in the initial phases of the program, but the results were not too encouraging. […} They decided to test the hypothesis that seeing the films and then discussing them would improve the viewers’ attitudes toward minority group members more than would just seeing the films. For a preliminary study the staff randomly selected a group of students from the total student body and attempted to pair the students on intelligence and socioeconomic background until ten pairs were obtained […] Each member of each pair was randomly assigned to either an experimental or a control group, and then both groups were shown a new film on intergroup relations. The A1 (experimental) group had a discission session after the picture was shown; the A2 (control) group had no such discussion afrer the film. Both groups were tested with a scale designed to measures attitudes toward minority groups.” Table 4.9: Data for One-Way Repeated-Measures ANOVA A1.experimental. A2.control. 8 6 9 8 5 3 4 2 2 1 10 7 3 1 12 7 6 6 11 9 4.4.1 Results Overview Table 4.10: Result Overview One-Way Repeated-Measures ANOVA By Hand JASP SPSS SAS Minitab R F 22.47 22.5 22.5 22.5 22.5 22.5 4.4.2 By Hand Calculations by hand can be found in Kerlinger, 1969, pp. 242-248. Results: Table 4.11: Result from Kerlinger (1969) Source df SS MS F Within groups 1 20 20.00 22.47 Between groups 9 182 20.22 22.72 Residual 9 8 0.89 NA Totals 19 210 NA NA Conclusion: All F-values are significant for \\(\\alpha\\) = .001 or less. 4.4.3 JASP Figure 4.13: JASP Output for One-Way Repeated-Measures ANOVA 4.4.4 SPSS DATASET ACTIVATE DataSet2. GLM A1.experimental A2.control /WSFACTOR=Condition 2 Polynomial /METHOD=SSTYPE(3) /CRITERIA=ALPHA(.05) /WSDESIGN=Condition. Figure 4.14: SPSS Output for One-Way Repeated-Measures ANOVA 4.4.5 SAS PROC GLM DATA=work.ranova; CLASS Subject Condition; MODEL Score = Subject Condition; RUN; Figure 4.15: SAS Output for One-Way Repeated-Measures ANOVA 4.4.6 Minitab Figure 4.16: Minitab Output for One-Way Repeated-Measures ANOVA 4.4.7 R summary(aov(Score ~ as.factor(ranova.data2$Condition) + Error(as.factor(ranova.data2$Subject)/as.factor(ranova.data2$Condition)), data=ranova.data2)) ## ## Error: as.factor(ranova.data2$Subject) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 9 182 20.22 ## ## Error: as.factor(ranova.data2$Subject):as.factor(ranova.data2$Condition) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(ranova.data2$Condition) 1 20 20.000 22.5 0.00105 ** ## Residuals 9 8 0.889 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.4.8 Remarks All differences in results between the software and hand calculation are due to rounding. 4.4.9 References Kerlinger, F. N. (1969). Foundations of behavioral research. New York, US: Holt, Rinehart and Winston, Inc. 4.5 Friedman Test An example from Hays (1974, pp. 785-786): “For example, in an experiment with four experimental treatments (J = 4), 11 groups of 4 matched subjects apiece were used. Within each matched group the four subjects were assigned at random to the four treatments, on subject per treatment.” Table 4.12: Data for Friedman Test Groups Treatment.I Treatment.II Treatment.III Treatment.IV 1 1 4 8 0 2 2 3 13 1 3 10 0 11 3 4 12 11 13 10 5 1 3 10 0 6 10 3 11 9 7 4 12 10 11 8 10 4 5 3 9 10 4 9 3 10 14 4 7 2 11 3 2 4 13 4.5.1 Results Overview Table 4.13: Result Overview Friedman Test By Hand JASP SPSS SAS Minitab R \\(\\chi^2\\) 11.79 11.9455 11.945 NA 11.95 11.945 4.5.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 785-786. Result: \\(\\chi^2\\) = 11.79 Significant for \\(\\alpha\\) = .01 or less. 4.5.3 JASP Figure 4.17: JASP Output for Friedman Test 4.5.4 SPSS DATASET ACTIVATE DataSet1. *Nonparametric Tests: Related Samples. NPTESTS /RELATED TEST(Treatment.I Treatment.II Treatment.III Treatment.IV) FRIEDMAN(COMPARE=NONE) /MISSING SCOPE=ANALYSIS USERMISSING=EXCLUDE /CRITERIA ALPHA=0.05 CILEVEL=95. Figure 4.18: SPSS Output for Friedman Test 4.5.5 SAS Not available in SAS. 4.5.6 Minitab Figure 4.19: Minitab Output for Friedman Test 4.5.7 R friedman.test(Friedman.data2$Score, groups = Friedman.data2$Treatment, blocks = Friedman.data2$Groups) ## ## Friedman rank sum test ## ## data: Friedman.data2$Score, Friedman.data2$Treatment and Friedman.data2$Groups ## Friedman chi-squared = 11.945, df = 3, p-value = 0.007572 4.5.8 Remarks All differences in results between the software and hand calculation are due to rounding. 4.5.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 4.6 ANCOVA An example from Field (2018 pp. 522-523, 576-577): “Imagine we wanted to contribute to this literature by running a study in which we randomized people into three groups: (1) a control group […]; (2) 15 minutes of puppy therapy (a low-dose group); and (3) 30 minutes of puppy contact (a high-dose group). The dependent variable was a measure of happiness ranging from 0 (as unhappy as I can possibly imagine being) to 10 (as happy as I can possibly imagine being). […] The researchers […] suddenly realized that a participant’s love of dogs would affect whether puppy therapy would affect happiness. Therefore, they repeated the study on different participants, but included a self-report measure of love of puppies from 0 […] to 7.” Table 4.14: Data for ANCOVA Person Dose Happiness Puppy_love 1 1 3 4 2 1 2 1 3 1 5 5 4 1 2 1 5 1 2 2 6 1 2 2 7 1 7 7 8 1 2 4 9 1 4 5 10 2 7 5 11 2 5 3 12 2 3 1 13 2 4 2 14 2 4 2 15 2 7 6 16 2 5 4 17 2 4 2 18 3 9 1 19 3 2 3 20 3 6 5 4.6.1 Results Overview Table 4.15: Result Overview ANCOVA JASP SPSS SAS Minitab R F (Dose) 4.1419 4.142 4.14 4.14 4.1419 F (Puppy_love) 4.9587 4.959 4.96 4.96 4.9587 4.6.2 JASP Figure 4.20: JASP Output for ANCOVA 4.6.3 SPSS UNIANOVA Happiness BY Dose WITH Puppy_love /CONTRAST(Dose)=Simple /METHOD=SSTYPE(3) /INTERCEPT=INCLUDE /PRINT ETASQ /CRITERIA=ALPHA(.05) /DESIGN=Puppy_love Dose. Figure 4.21: SPSS Output for ANCOVA Figure 4.22: SPSS Output for ANCOVA Contrasts 4.6.4 SAS PROC GLM data=work.ANCOVA; CLASS Dose; MODEL Happiness = Dose Puppy_love / SOLUTION ss3; LSMEANS Dose / STDERR PDIFF CL ADJUST = BON; OUTPUT OUT = pred p=ybar r=resid; RUN; Figure 4.23: SAS Output for ANCOVA 4.6.5 Minitab Figure 4.24: Minitab Output for ANCOVA 4.6.6 R ## install.packages(&quot;car&quot;) library(&quot;car&quot;) ## Loading required package: carData fit &lt;- aov(Happiness~as.factor(Dose) + Puppy_love, data = ANCOVA.data) Anova(fit, type = &quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: Happiness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 12.943 1 4.2572 0.04920 * ## as.factor(Dose) 25.185 2 4.1419 0.02745 * ## Puppy_love 15.076 1 4.9587 0.03483 * ## Residuals 79.047 26 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.6.7 Remarks All differences in results between the software are due to rounding. 4.6.8 References Field, A. (2018). Discovering statistics using IBM SPSS statistics. Los Angeles, CA: SAGE. 4.7 MANOVA An example from Field (2018 pp. 738-739): “Most psychopathologies have both behavioural and cognitive elements to them. For example, for someone with OCD who has an obsession with germs and contamination, the disorder might manifest itself in the number of times they both wash their hands (behavior) and think about washing their hands (cognition). To gauge the success of therapy, it is not enough to look only at behavioural outcomes (such as whether obsessive behaviours are reduced); we need to look at whether cognitions are changed too. Hence, the clinical psychologist measured two outcomes: the occurrence of obsession-related behaviours (Actions) and the occurrence of obsession-related cognitions (Thoughts) on a single day.” Table 4.16: Data for MANOVA Group Actions Thoughts 1 5 14 1 5 11 1 4 16 1 4 13 1 5 12 1 3 14 1 7 12 1 6 15 1 6 16 1 4 11 2 4 14 2 4 15 2 1 13 2 1 14 2 4 15 2 6 19 2 5 13 2 5 18 2 2 14 2 5 17 4.7.1 Results Overview Table 4.17: Result Overview MANOVA JASP SPSS SAS Minitab R F (Multivariate Pillai) 2.5567 2.557 2.56 2.557 2.5567 F (Univariate Actions) 2.7706 2.771 2.77 2.770 2.7706 F (Univariate Thoughts) 2.1541 2.154 2.15 2.150 2.1541 4.7.2 JASP Figure 4.25: JASP Output for MANOVA 4.7.3 SPSS DATASET ACTIVATE DataSet2. GLM Actions Thoughts BY Group /METHOD=SSTYPE(3) /INTERCEPT=INCLUDE /CRITERIA=ALPHA(.05) /DESIGN= Group. Figure 4.26: SPSS Output for MANOVA Figure 4.27: SPSS Output for MANOVA 4.7.4 SAS proc glm data= work.MANOVA plots=none; class Group; model Actions Thoughts = group / ss3; manova h=Group; run; Figure 4.28: SAS Output for MANOVA Figure 4.29: SAS Output for MANOVA 4.7.5 Minitab Figure 4.30: Minitab Output for MANOVA Figure 4.31: Minitab Output for MANOVA 4.7.6 R DVmanova &lt;- cbind(MANOVA.data$Thoughts, MANOVA.data$Actions) fit &lt;- manova(DVmanova ~ as.factor(Group), data = MANOVA.data) summary(fit, test =&quot;Pillai&quot;) ## Df Pillai approx F num Df den Df Pr(&gt;F) ## as.factor(Group) 2 0.31845 2.5567 4 54 0.04904 * ## Residuals 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fit, test =&quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## as.factor(Group) 2 0.69851 2.5545 4 52 0.04966 * ## Residuals 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fit, test =&quot;Hotelling-Lawley&quot;) ## Df Hotelling-Lawley approx F num Df den Df Pr(&gt;F) ## as.factor(Group) 2 0.40734 2.5459 4 50 0.0508 . ## Residuals 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fit, test =&quot;Roy&quot;) ## Df Roy approx F num Df den Df Pr(&gt;F) ## as.factor(Group) 2 0.3348 4.5198 2 27 0.02027 * ## Residuals 27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary.aov(fit) ## Response 1 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(Group) 2 19.467 9.7333 2.1541 0.1355 ## Residuals 27 122.000 4.5185 ## ## Response 2 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(Group) 2 10.467 5.2333 2.7706 0.08046 . ## Residuals 27 51.000 1.8889 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.7.7 Remarks All differences in results between the software are due to rounding. 4.7.8 References Field, A. (2018). Discovering statistics using IBM SPSS statistics. Los Angeles, CA: SAGE. "],["mixed-models.html", "Chapter 5 Mixed Models 5.1 Linear Mixed Models", " Chapter 5 Mixed Models 5.1 Linear Mixed Models The example is based on data from an Experiment performed by Freeman, Heathcote, Chalmers, and Hockley (2010). In the example a simplified analysis of the data is performed, using only Task and Stimulus as fixed factors and stimulus by subject as random effect. 5.1.1 Results Overview Table 5.1: Result Overview Linear Mixed Model JASP SPSS SAS Minitab R F(Task) 15.0731 15.075 15.07 15.08 15.07 F(Stimulus) 88.7032 88.716 88.70 88.72 88.70 F(Task X Stimulus) 37.8429 37.846 37.84 37.85 37.84 5.1.2 JASP Figure 5.1: JASP Output for Linear Mixed Model 5.1.3 SPSS DATASET ACTIVATE DataSet1. MIXED rt BY task stimulus WITH NumID /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(100) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE) /FIXED=task stimulus task*stimulus | NOINT SSTYPE(3) /METHOD=REML /RANDOM=INTERCEPT stimulus | SUBJECT(NumID) COVTYPE(VC). Figure 5.2: SPSS Output for Linear Mixed Model 5.1.4 SAS proc mixed; class Task Stimulus; model rt = Task Stimulus Task*Stimulus / ddfm=satterth; random intercept Stimulus / type=un subject=NumID g; run; Figure 5.3: SAS Output for Linear Mixed Model 5.1.5 Minitab Figure 5.4: Minitab Output for Linear Mixed Model 5.1.6 R #install.packages(&quot;afex&quot;) library(&quot;afex&quot;) ## Loading required package: lme4 ## Loading required package: Matrix ## ************ ## Welcome to afex. For support visit: http://afex.singmann.science/ ## - Functions for ANOVAs: aov_car(), aov_ez(), and aov_4() ## - Methods for calculating p-values with mixed(): &#39;S&#39;, &#39;KR&#39;, &#39;LRT&#39;, and &#39;PB&#39; ## - &#39;afex_aov&#39; and &#39;mixed&#39; objects can be passed to emmeans() for follow-up tests ## - NEWS: emmeans() for ANOVA models now uses model = &#39;multivariate&#39; as default. ## - Get and set global package options with: afex_options() ## - Set orthogonal sum-to-zero contrasts globally: set_sum_contrasts() ## - For example analyses see: browseVignettes(&quot;afex&quot;) ## ************ ## ## Attaching package: &#39;afex&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer set_sum_contrasts() ## not strictly necessary ## setting contr.sum globally: options(contrasts=c(&#39;contr.sum&#39;, &#39;contr.poly&#39;)) ### but always a good idea m1 &lt;- mixed(rt ~ task*stimulus + (stimulus|id), data = LMM.data, method=&quot;S&quot;) ## Fitting one lmer() model. [DONE] ## Calculating p-values. [DONE] m1 ## Mixed Model Anova Table (Type 3 tests, S-method) ## ## Model: rt ~ task * stimulus + (stimulus | id) ## Data: LMM.data ## Effect df F p.value ## 1 task 1, 42.98 15.07 *** &lt;.001 ## 2 stimulus 1, 42.89 88.70 *** &lt;.001 ## 3 task:stimulus 1, 42.89 37.84 *** &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 5.1.7 Remarks All differences in results between the software are due to rounding. 5.1.8 References Freeman, E., Heathcote, A., Chalmers, K., &amp; Hockley, W. (2010). Item effects in recognition memory for words. Journal of Memory and Language, 62(1), 1-18. "],["regression.html", "Chapter 6 Regression 6.1 Correlation 6.2 Linear Regression 6.3 Logistic Regression", " Chapter 6 Regression 6.1 Correlation An example from Hays (1974, pp. 633-635): “The teacher collected data for a class of 91 students, obtaining for each a score X, based on the number of courses in high-school mathematics, and a score Y, the actual score on the final examination for the course.” Table 6.1: Data for Correlation X Y 2.0 22 2.0 17 2.0 16 2.0 14 2.0 10 2.0 9 2.0 7 2.0 5 2.0 3 2.0 2 2.5 26 2.5 23 2.5 18 2.5 18 2.5 16 2.5 13 2.5 12 2.5 10 2.5 10 2.5 7 2.5 6 3.0 29 3.0 26 3.0 26 3.0 24 3.0 24 3.0 23 3.0 22 3.0 16 3.0 9 3.0 8 3.5 34 3.5 26 3.5 25 3.5 23 3.5 23 3.5 22 3.5 22 3.5 19 3.5 18 3.5 17 3.5 17 3.5 17 3.5 12 3.5 8 4.0 36 4.0 35 4.0 30 4.0 27 4.0 25 4.0 25 4.0 24 4.0 21 4.0 20 4.0 19 4.0 19 4.0 18 4.0 18 4.0 12 4.0 3 4.5 28 4.5 27 4.5 16 5.0 41 5.0 32 5.0 27 5.0 19 5.5 32 5.5 25 5.5 25 6.0 46 6.0 38 6.0 34 6.0 33 6.0 27 6.0 20 6.5 44 6.5 37 6.5 32 6.5 28 7.0 52 7.0 46 7.0 37 7.5 42 7.5 41 7.5 38 7.5 35 8.0 53 8.0 48 8.0 40 8.0 40 6.1.1 Results Overview Table 6.2: Result Overview Correlation By Hand JASP SPSS SAS Minitab R Pearson 0.81 0.8064 0.806 0.8064 0.806 0.8064 Spearman NA 0.7730 0.773 0.7730 0.773 0.7730 Kendall NA 0.6121 0.612 0.6120 NA 0.6120 6.1.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 633-635. Result: r = 0.81 Note: Hays calculated only the Pearson correlation coefficient. 6.1.3 JASP Figure 6.1: JASP Output for Correlation 6.1.4 SPSS DATASET ACTIVATE DataSet1. CORRELATIONS /VARIABLES=Y X /PRINT=TWOTAIL NOSIG /MISSING=PAIRWISE. NONPAR CORR /VARIABLES=Y X /PRINT=BOTH TWOTAIL NOSIG /MISSING=PAIRWISE. Figure 6.2: SPSS Output for Correlation 6.1.5 SAS PROC CORR DATA=Correlation pearson spearman kendall; VAR X; WITH Y; RUN; Figure 6.3: SAS Output for Correlation 6.1.6 Minitab Figure 6.4: Minitab Output for Pearson Correlation Figure 6.5: Minitab Output for Spearman Correlation 6.1.7 R cor.test(corr.data$X, corr.data$Y, method =&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: corr.data$X and corr.data$Y ## t = 12.866, df = 89, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7200843 0.8681910 ## sample estimates: ## cor ## 0.8064365 cor.test(corr.data$X, corr.data$Y, method =&quot;spearman&quot;) ## Warning in cor.test.default(corr.data$X, corr.data$Y, method = &quot;spearman&quot;): ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: corr.data$X and corr.data$Y ## S = 28502, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7730333 cor.test(corr.data$X, corr.data$Y, method =&quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: corr.data$X and corr.data$Y ## z = 8.1519, p-value = 3.582e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.6120845 6.1.8 Remarks All differences in results between the software and hand calculation are due to rounding. 6.1.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 6.2 Linear Regression An example from Hays (1974, pp. 633-635): “The teacher collected data for a class of 91 students, obtaining for each a score X, based on the number of courses in high-school mathematics, and a score Y, the actual score on the final examination for the course.” Table 6.3: Data for Regression X Y 2.0 22 2.0 17 2.0 16 2.0 14 2.0 10 2.0 9 2.0 7 2.0 5 2.0 3 2.0 2 2.5 26 2.5 23 2.5 18 2.5 18 2.5 16 2.5 13 2.5 12 2.5 10 2.5 10 2.5 7 2.5 6 3.0 29 3.0 26 3.0 26 3.0 24 3.0 24 3.0 23 3.0 22 3.0 16 3.0 9 3.0 8 3.5 34 3.5 26 3.5 25 3.5 23 3.5 23 3.5 22 3.5 22 3.5 19 3.5 18 3.5 17 3.5 17 3.5 17 3.5 12 3.5 8 4.0 36 4.0 35 4.0 30 4.0 27 4.0 25 4.0 25 4.0 24 4.0 21 4.0 20 4.0 19 4.0 19 4.0 18 4.0 18 4.0 12 4.0 3 4.5 28 4.5 27 4.5 16 5.0 41 5.0 32 5.0 27 5.0 19 5.5 32 5.5 25 5.5 25 6.0 46 6.0 38 6.0 34 6.0 33 6.0 27 6.0 20 6.5 44 6.5 37 6.5 32 6.5 28 7.0 52 7.0 46 7.0 37 7.5 42 7.5 41 7.5 38 7.5 35 8.0 53 8.0 48 8.0 40 8.0 40 6.2.1 Results Overview Table 6.4: Result Overview Independent Factorial ANOVA By Hand JASP SPSS SAS Minitab R Constant 23.84 23.8352 23.835 23.8352 23.835 23.8350 Regression Coefficient 5.42 5.3993 5.399 5.3993 5.399 5.3990 \\(\\overline{x}\\) 4.19 4.1923 4.192 4.1923 4.192 4.1923 6.2.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 633-635. Results: y’ = (5.42)(x - 4.19) + 23.84 Note: Hays mean-centered the equation by the mean of \\(\\overline{x}\\) = 4.19. 6.2.3 JASP Figure 6.6: JASP Output for Regression Figure 6.7: JASP Output for Descriptives Mean-centered regression equation: y’ = (5.3993)(x - 4.1923) + 23.8352 6.2.4 SPSS DESCRIPTIVES VARIABLES=X /STATISTICS=MEAN. REGRESSION /DESCRIPTIVES MEAN STDDEV CORR SIG N /MISSING LISTWISE /STATISTICS COEFF OUTS R ANOVA /CRITERIA=PIN(.05) POUT(.10) /NOORIGIN /DEPENDENT Y /METHOD=ENTER MeanCenteredX. Figure 6.8: SPSS Output for Regression Mean-centered regression equation: y’ = (5.399)(x - 4.192) + 23.835 6.2.5 SAS proc Reg data=Regression; title &quot;Linear regression&quot;; model Y = MeanCenteredX; run; PROC MEANS DATA=Regression; VAR X Y; RUN; Figure 6.9: SAS Output for Regression Figure 6.10: SAS Output for Means Mean-centered regression equation: y’ = (5.3993)(x - 4.1923077) + 23.83516 6.2.6 Minitab Figure 6.11: Minitab Output for Regression Figure 6.12: Minitab Output for Means Mean-centered regression equation: y’ = (5.399)(x - 4.192) + 23.835 6.2.7 R regress.data2 &lt;- read.csv(&quot;Datasets/Regression.csv&quot;, sep=&quot;,&quot;) lm(formula = Y ~ MeanCenteredX, data = regress.data2) ## ## Call: ## lm(formula = Y ~ MeanCenteredX, data = regress.data2) ## ## Coefficients: ## (Intercept) MeanCenteredX ## 23.835 5.399 mean(regress.data2$X) ## [1] 4.192308 Mean-centered regression equation: y’ = (5.399)(x - 4.192308) + 23.835 6.2.8 Remarks All differences in results between the software and hand calculation are due to rounding. 6.2.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 6.3 Logistic Regression An example: The Titanic-dataset contains original data of all passengers of the Titanic. It contains their name, their passenger class (1st - 3rd), their age, their sex, and whether or not they survived the sinking of the ship. The logistic regression model is computed to allow predictions on a passengers survival status, based on their age, sex, and passenger class. 6.3.1 Results Overview Table 6.5: Result Overview Coefficents Logistic Regression JASP SPSS SAS Minitab R Constant 3.7597 3.760 3.7596 3.7600 3.7597 Age -0.0392 -0.039 -0.0392 -0.0392 -0.0392 2nd Class -1.2920 -1.292 -1.2920 -1.2920 -1.2920 3rd Class -2.5214 -2.521 -2.5214 -2.5210 -2.5214 Sex(Male) -2.6314 -2.631 -2.6313 -2.6310 -2.6314 Table 6.6: Result Overview Odds-ratio Logistic Regression JASP SPSS SAS Minitab R Constant 42.9339 42.934 NA NA 42.9339 Age 0.9616 0.962 0.962 0.9616 0.9616 2nd Class 0.2747 0.275 0.275 0.2747 0.2747 3rd Class 0.0803 0.080 0.080 0.0803 0.0803 Sex(Male) 0.0720 0.072 0.072 0.0720 0.0720 Note: The reference case to which the odds ratios are refering is a female passenger in the first class with age 0. 6.3.2 JASP Figure 6.13: JASP Output for Logistic Regression 6.3.3 SPSS DATASET ACTIVATE DataSet1. LOGISTIC REGRESSION VARIABLES Survived /METHOD=ENTER PClass Age Sex /CONTRAST (PClass)=Indicator(1) /CONTRAST (Sex)=Indicator(1) /CRITERIA=PIN(.05) POUT(.10) ITERATE(20) CUT(.5). Figure 6.14: SPSS Output for Logistic Regression 6.3.4 SAS proc logistic data=work.LogReg DESC; class PClass Sex / param=reference ref=first; model Survived = Age Sex PClass; run; Figure 6.15: SAS Output for Logistic Regression 6.3.5 Minitab Figure 6.16: Minitab Output for Logistic Regression 6.3.6 R LogRegExample &lt;- glm(factor(Survived) ~Age + factor(PClass) + factor(Sex), data=LogReg.data2, family=binomial(link=&quot;logit&quot;)) summary(LogRegExample) ## ## Call: ## glm(formula = factor(Survived) ~ Age + factor(PClass) + factor(Sex), ## family = binomial(link = &quot;logit&quot;), data = LogReg.data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7226 -0.7065 -0.3917 0.6495 2.5289 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.172856 0.253988 4.618 3.88e-06 *** ## Age -0.039177 0.007616 -5.144 2.69e-07 *** ## factor(PClass)1 1.271127 0.160563 7.917 2.44e-15 *** ## factor(PClass)2 -0.020835 0.138004 -0.151 0.88 ## factor(Sex)1 1.315678 0.100753 13.058 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1025.57 on 755 degrees of freedom ## Residual deviance: 695.14 on 751 degrees of freedom ## (557 observations deleted due to missingness) ## AIC: 705.14 ## ## Number of Fisher Scoring iterations: 5 exp(coef(LogRegExample)) ## (Intercept) Age factor(PClass)1 factor(PClass)2 factor(Sex)1 ## 3.2312094 0.9615807 3.5648686 0.9793803 3.7272788 6.3.7 Remarks All differences in results between the software are due to rounding. "],["frequencies.html", "Chapter 7 Frequencies 7.1 Binomial Test 7.2 Multinomial Test / Chi-square Goodness of Fit Test 7.3 Chi-Squared-Test", " Chapter 7 Frequencies 7.1 Binomial Test 7.1.1 Example An example from Hays (1974, pp. 190-192): “Think of a hypothetical study of this question: ‘If a human is subjected to a stimulus below his threshold of conscious awareness, can his behavior somehow still be influenced by the presence of the stimulus?’ The experimental task is as follows: the subject is seated in a room in front of a square screen divided into four equal parts. He is instructed that his task is to guess in which part of the screen a small, very faint, spot of light is thrown.” Under the null hypothesis H0, the number of correct guesses is expected to be 1/4 of the trials N. The alternative hypothesis H1 is that the number of correct guesses is larger than 1/4 of the trials N. The subject obtained 7 correct guesses T out of 10 trials N. What is the p-value of this result under H0? p = 0.25 N = 10 T = 7 7.1.2 Results Overview Table 7.1: Result Overview Binomial Test By Hand JASP SPSS SAS Minitab R P 0.0035 0.0035 0.004 0.0035 0.004 0.0035 7.1.3 By Hand Calculations by hand can be found in Hays, 1974, pp. 190-192. Result: P = 0.0035 7.1.4 JASP Figure 7.1: JASP Output for Binomial Test 7.1.5 SPSS DATASET NAME DataSet1 WINDOW=FRONT. *Nonparametric Tests: One Sample. NPTESTS /ONESAMPLE TEST (Guesses) BINOMIAL(TESTVALUE=0.25 SUCCESSCATEGORICAL=FIRST SUCCESSCONTINUOUS=CUTPOINT(MIDPOINT)) /MISSING SCOPE=ANALYSIS USERMISSING=EXCLUDE /CRITERIA ALPHA=0.05 CILEVEL=95. Figure 7.2: SPSS Output for Binomial Test 7.1.6 SAS PROC Freq data=WORK.IMPORT; tables Guesses / binomial(p=.25 level=2); exact binomial; run; Figure 7.3: SAS Output for Binomial Test 7.1.7 Minitab Figure 7.4: Minitab Output for Binomial Test 7.1.8 R binom.test(7, 10, p = 0.25, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 7 and 10 ## number of successes = 7, number of trials = 10, p-value = 0.003506 ## alternative hypothesis: true probability of success is greater than 0.25 ## 95 percent confidence interval: ## 0.3933758 1.0000000 ## sample estimates: ## probability of success ## 0.7 7.1.9 Remarks All differences in results between the software and hand calculation are due to rounding. 7.1.10 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. 7.2 Multinomial Test / Chi-square Goodness of Fit Test 7.2.1 Example Think of colored marbles mixed together in a box, where the following probability distribution holds: Table 7.2: Probability Distribution for Multinomial Test Example Color p Black 0.4 Red 0.3 White 0.3 Now suppose that 10 marbles were drawn at random and with replacement. The samples shows 2 black, 3 red, and 5 white. Table 7.3: Sample for Multinomial Test Example Color Count Expected Black 2 4 Red 3 3 White 5 3 7.2.2 Results Overview Table 7.4: Result Overview Multinomial Test JASP SPSS SAS Minitab R \\(\\chi ^2\\) 2.333 2.333 2.333 2.333 2.333 7.2.3 JASP Figure 7.5: JASP Output for Multinomial Test 7.2.4 SPSS DATASET ACTIVATE DataSet1. NPAR TESTS /CHISQUARE=Numbered /EXPECTED=4 3 3 /MISSING ANALYSIS. Figure 7.6: SPSS Output for Multinomial Test 7.2.5 SAS PROC FREQ DATA = chisquared; TABLES Sex*Preference / chisq; run; Figure 7.7: SAS Output for Multinomial Test 7.2.6 Minitab Figure 7.8: Minitab Output for Multinomial Test 7.2.7 R chisq.test(MNsample$Count, p = Pdist$p) ## Warning in chisq.test(MNsample$Count, p = Pdist$p): Chi-squared approximation ## may be incorrect ## ## Chi-squared test for given probabilities ## ## data: MNsample$Count ## X-squared = 2.3333, df = 2, p-value = 0.3114 7.2.8 Remarks All differences in results between the software are due to rounding. 7.3 Chi-Squared-Test An example from Hays (1974, pp. 728-731): “For example, suppose that a random sample of 1– school children is drawn. Each child is classified in two ways: the first attribute is the sex of the child, with two possible categories: [Male, Female]. The second attribute […] is the stated preference of a child for two kinds of reading materials: [Fiction, Nonfiction]. […] The data might, for example, turn out to be:” Table 7.5: Data for Chi-Squared-Test Male Female Fiction 19 32 Nonfiction 29 20 7.3.1 Results Overview Table 7.6: Result Overview Chi-Squared-Test By Hand JASP SPSS SAS Minitab R \\(\\chi ^2\\) 4.83 4.8145 4.814 4.8145 4.814 4.8145 7.3.2 By Hand Calculations by hand can be found in Hays, 1974, pp. 728-731. Result: \\(\\chi ^2\\) = 4.83 Significant for \\(\\alpha\\) = .05 or less 7.3.3 JASP Figure 7.9: JASP Output for Chi-Squared-Test 7.3.4 SPSS CROSSTABS /TABLES=Sex BY Preference /FORMAT=AVALUE TABLES /STATISTICS=CHISQ /CELLS=COUNT /COUNT ROUND CELL. Figure 7.10: SPSS Output for Chi-Squared-Test 7.3.5 SAS PROC FREQ DATA = chisquared; TABLES Sex*Preference / chisq; run; Figure 7.11: SAS Output for Chi-Squared-Test 7.3.6 Minitab Figure 7.12: Minitab Output for Chi-Squared-Test 7.3.7 R chisq.test(chiSquare.data, correct = F) ## ## Pearson&#39;s Chi-squared test ## ## data: chiSquare.data ## X-squared = 4.8145, df = 1, p-value = 0.02822 7.3.8 Remarks All differences in results between the software and hand calculation are due to rounding. 7.3.9 References Hays, W. L. (1974). Statistics for the social sciences (2nd Ed.). New York, US: Holt, Rinehart and Winston, Inc. "],["factor.html", "Chapter 8 Factor 8.1 Principal Component Analysis 8.2 Exploratory Factor Analysis", " Chapter 8 Factor 8.1 Principal Component Analysis An example from Field (2018 pp. 795-796): “I have noticed that a lot of students become very stressed about SPSS Statistics. Imagine that I wanted to design a questionnaire to measure a trait that I termed ‘SPSS anxiety’. I devised a questionnaire to measure various aspects of students’ anxiety towards learning SPSS, the SAQ. I generated questions based on interviews with anxious and non-anxious students and came up with 23 possible questions to include. Each question was a statement followed by a five-point Likert scale: ‘strongly disagree’, ‘disagree’, ‘neither agree nor disagree’, ‘agree’ and ‘strongly agree’ (SD, D, N, A and SA, respectively). What’s more, I wanted to know whether anxiety about SPSS could be broken down into specific forms of anxiety. In other words, what latent variables contribute to anxiety about SPSS? With a little help from a few lecturer friends I collected 2571 completed questionnaires.” 8.1.1 Results Overview Table 8.1: Result Overview Exploratory Factor Analysis JASP SPSS SAS Minitab R SS Loading (Factor1) 3.0336 3.033 3.034 NA 3.03 SS Loading (Factor2) 2.8545 2.855 2.855 NA 2.85 SS Loading (Factor3) 1.9859 1.986 1.986 NA 1.99 SS Loading (Factor4) 1.4351 1.435 1.435 NA 1.44 8.1.2 JASP Figure 8.1: JASP Output for Principal Component Analysis 8.1.3 SPSS DATASET ACTIVATE DataSet1. FACTOR /VARIABLES Question_01 Question_02 Question_03 Question_04 Question_05 Question_06 Question_07 Question_08 Question_09 Question_10 Question_11 Question_12 Question_13 Question_14 Question_15 Question_16 Question_17 Question_18 Question_19 Question_20 Question_21 Question_22 Question_23 /MISSING LISTWISE /ANALYSIS Question_01 Question_02 Question_03 Question_04 Question_05 Question_06 Question_07 Question_08 Question_09 Question_10 Question_11 Question_12 Question_13 Question_14 Question_15 Question_16 Question_17 Question_18 Question_19 Question_20 Question_21 Question_22 Question_23 /PRINT INITIAL ROTATION /PLOT EIGEN /CRITERIA MINEIGEN(1) ITERATE(25) /EXTRACTION PC /CRITERIA ITERATE(25) /ROTATION VARIMAX /METHOD=CORRELATION. Figure 8.2: SPSS Output for Principal Component Analysis Figure 8.3: SPSS Output for Principal Component Analysis 8.1.4 SAS PROC FACTOR Data=work.PCA scree Nfactors= 4 Method= p Rotate=varimax; Var Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 Q21 Q22 Q23 ; Run; Figure 8.4: SAS Output for Principal Component Analysis Figure 8.5: SAS Output for Principal Component Analysis Figure 8.6: SAS Output for Principal Component Analysis 8.1.5 Minitab Figure 8.7: Minitab Output for Principal Component Analysis Figure 8.8: Minitab Output for Principal Component Analysis Figure 8.9: Minitab Output for Principal Component Analysis 8.1.6 R ## install.packages(&quot;psych&quot;) ## install.packages(&quot;factoextra&quot;) ## Principal Component Analysis library(&quot;psych&quot;) ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:car&#39;: ## ## logit library(&quot;factoextra&quot;) ## Loading required package: ggplot2 ## ## Attaching package: &#39;ggplot2&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## %+%, alpha ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa fit1 &lt;- prcomp(PCA.data, scale = TRUE) #eigenvalues eig.val &lt;- get_eigenvalue(fit1) eig.val ## print results ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 7.2900471 31.695857 31.69586 ## Dim.2 1.7388287 7.560125 39.25598 ## Dim.3 1.3167515 5.725007 44.98099 ## Dim.4 1.2271982 5.335644 50.31663 ## Dim.5 0.9878779 4.295121 54.61175 ## Dim.6 0.8953304 3.892741 58.50449 ## Dim.7 0.8055604 3.502436 62.00693 ## Dim.8 0.7828199 3.403565 65.41050 ## Dim.9 0.7509712 3.265092 68.67559 ## Dim.10 0.7169577 3.117207 71.79280 ## Dim.11 0.6835877 2.972121 74.76492 ## Dim.12 0.6695016 2.910876 77.67579 ## Dim.13 0.6119976 2.660859 80.33665 ## Dim.14 0.5777377 2.511903 82.84855 ## Dim.15 0.5491875 2.387772 85.23633 ## Dim.16 0.5231504 2.274567 87.51089 ## Dim.17 0.5083962 2.210418 89.72131 ## Dim.18 0.4559399 1.982347 91.70366 ## Dim.19 0.4238036 1.842624 93.54628 ## Dim.20 0.4077909 1.773004 95.31929 ## Dim.21 0.3794799 1.649912 96.96920 ## Dim.22 0.3640223 1.582705 98.55191 ## Dim.23 0.3330618 1.448095 100.00000 #fviz_eig(fit1) #scree plot fit2 &lt;- principal(PCA.data, nfactors=4, rotate = &quot;varimax&quot;) #varimax rotiation fit2 ## print results ## Principal Components Analysis ## Call: principal(r = PCA.data, nfactors = 4, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC3 RC1 RC4 RC2 h2 u2 com ## Question_01 0.24 0.50 0.36 0.06 0.43 0.57 2.4 ## Question_02 -0.01 -0.34 0.07 0.54 0.41 0.59 1.7 ## Question_03 -0.20 -0.57 -0.18 0.37 0.53 0.47 2.3 ## Question_04 0.32 0.52 0.31 0.04 0.47 0.53 2.4 ## Question_05 0.32 0.43 0.24 0.01 0.34 0.66 2.5 ## Question_06 0.80 -0.01 0.10 -0.07 0.65 0.35 1.0 ## Question_07 0.64 0.33 0.16 -0.08 0.55 0.45 1.7 ## Question_08 0.13 0.17 0.83 0.01 0.74 0.26 1.1 ## Question_09 -0.09 -0.20 0.12 0.65 0.48 0.52 1.3 ## Question_10 0.55 0.00 0.13 -0.12 0.33 0.67 1.2 ## Question_11 0.26 0.21 0.75 -0.14 0.69 0.31 1.5 ## Question_12 0.47 0.52 0.09 -0.08 0.51 0.49 2.1 ## Question_13 0.65 0.23 0.23 -0.10 0.54 0.46 1.6 ## Question_14 0.58 0.36 0.14 -0.07 0.49 0.51 1.8 ## Question_15 0.46 0.22 0.29 -0.19 0.38 0.62 2.6 ## Question_16 0.33 0.51 0.31 -0.12 0.49 0.51 2.6 ## Question_17 0.27 0.22 0.75 -0.04 0.68 0.32 1.5 ## Question_18 0.68 0.33 0.13 -0.08 0.60 0.40 1.5 ## Question_19 -0.15 -0.37 -0.03 0.43 0.34 0.66 2.2 ## Question_20 -0.04 0.68 0.07 -0.14 0.48 0.52 1.1 ## Question_21 0.29 0.66 0.16 -0.07 0.55 0.45 1.5 ## Question_22 -0.19 0.03 -0.10 0.65 0.46 0.54 1.2 ## Question_23 -0.02 0.17 -0.20 0.59 0.41 0.59 1.4 ## ## RC3 RC1 RC4 RC2 ## SS loadings 3.73 3.34 2.55 1.95 ## Proportion Var 0.16 0.15 0.11 0.08 ## Cumulative Var 0.16 0.31 0.42 0.50 ## Proportion Explained 0.32 0.29 0.22 0.17 ## Cumulative Proportion 0.32 0.61 0.83 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 4 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 4006.15 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.96 8.1.7 Remarks The rotation used was “Varimax”. All differences in results between the software are due to rounding. 8.1.8 References Field, A. (2018). Discovering statistics using IBM SPSS statistics. Los Angeles, CA: SAGE. 8.2 Exploratory Factor Analysis An example from Field (2018 pp. 795-796): “I have noticed that a lot of students become very stressed about SPSS Statistics. Imagine that I wanted to design a questionnaire to measure a trait that I termed ‘SPSS anxiety’. I devised a questionnaire to measure various aspects of students’ anxiety towards learning SPSS, the SAQ. I generated questions based on interviews with anxious and non-anxious students and came up with 23 possible questions to include. Each question was a statement followed by a five-point Likert scale: ‘strongly disagree’, ‘disagree’, ‘neither agree nor disagree’, ‘agree’ and ‘strongly agree’ (SD, D, N, A and SA, respectively). What’s more, I wanted to know whether anxiety about SPSS could be broken down into specific forms of anxiety. In other words, what latent variables contribute to anxiety about SPSS? With a little help from a few lecturer friends I collected 2571 completed questionnaires.” 8.2.1 Results Overview Table 8.2: Result Overview Exploratory Factor Analysis JASP SPSS SAS Minitab R SS Loading (Factor1) 3.0336 3.033 3.034 NA 3.03 SS Loading (Factor2) 2.8545 2.855 2.855 NA 2.85 SS Loading (Factor3) 1.9859 1.986 1.986 NA 1.99 SS Loading (Factor4) 1.4351 1.435 1.435 NA 1.44 8.2.2 JASP Figure 8.10: JASP Output for Exploratory Factor Analysis Figure 8.11: JASP Output for Exploratory Factor Analysis Figure 8.12: JASP Output for Exploratory Factor Analysis 8.2.3 SPSS FACTOR /VARIABLES Question_01 Question_02 Question_03 Question_04 Question_05 Question_06 Question_07 Question_08 Question_09 Question_10 Question_11 Question_12 Question_13 Question_14 Question_15 Question_16 Question_17 Question_18 Question_19 Question_20 Question_21 Question_22 Question_23 /MISSING LISTWISE /ANALYSIS Question_01 Question_02 Question_03 Question_04 Question_05 Question_06 Question_07 Question_08 Question_09 Question_10 Question_11 Question_12 Question_13 Question_14 Question_15 Question_16 Question_17 Question_18 Question_19 Question_20 Question_21 Question_22 Question_23 /PRINT INITIAL KMO EXTRACTION ROTATION /CRITERIA MINEIGEN(1) ITERATE(25) /EXTRACTION PAF /CRITERIA ITERATE(25) /ROTATION VARIMAX /METHOD=CORRELATION. Figure 8.13: SPSS Output for Exploratory Factor Analysis Figure 8.14: SPSS Output for Exploratory Factor Analysis 8.2.4 SAS PROC FACTOR Data=work.efa scree Nfactors= 4 Method= prinit Rotate=varimax; Var Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 Q21 Q22 Q23 ; Run; Figure 8.15: SAS Output for Exploratory Factor Analysis Figure 8.16: SAS Output for Exploratory Factor Analysis 8.2.5 Minitab Exploratory Factor Analysis with Principal Axis Factoring is not available in Minitab. 8.2.6 R ## install.packages(&quot;psych&quot;) ## Principal Axis Factor Analysis library(&quot;psych&quot;) fit &lt;- factor.pa(EFA.data, nfactors=4, rotate = &quot;varimax&quot;) ## Warning: factor.pa is deprecated. Please use the fa function with fm=pa fit ## print results ## Factor Analysis using method = pa ## Call: factor.pa(r = EFA.data, nfactors = 4, rotate = &quot;varimax&quot;) ## Unstandardized loadings (pattern matrix) based upon covariance matrix ## PA1 PA3 PA4 PA2 h2 u2 H2 U2 ## Question_01 0.50 0.22 -0.27 0.00 0.37 0.63 0.37 0.63 ## Question_02 -0.21 -0.03 -0.01 0.46 0.26 0.74 0.26 0.74 ## Question_03 -0.50 -0.18 0.16 0.40 0.47 0.53 0.47 0.53 ## Question_04 0.53 0.28 -0.25 -0.03 0.42 0.58 0.42 0.58 ## Question_05 0.44 0.27 -0.19 -0.05 0.30 0.70 0.30 0.70 ## Question_06 0.05 0.75 -0.12 -0.10 0.59 0.41 0.59 0.41 ## Question_07 0.36 0.56 -0.16 -0.13 0.49 0.51 0.49 0.51 ## Question_08 0.22 0.15 -0.76 0.00 0.65 0.35 0.65 0.35 ## Question_09 -0.13 -0.07 -0.06 0.56 0.34 0.66 0.34 0.66 ## Question_10 0.14 0.38 -0.14 -0.12 0.20 0.80 0.20 0.80 ## Question_11 0.24 0.27 -0.69 -0.17 0.63 0.37 0.63 0.37 ## Question_12 0.51 0.40 -0.11 -0.15 0.45 0.55 0.45 0.55 ## Question_13 0.29 0.56 -0.23 -0.14 0.47 0.53 0.47 0.53 ## Question_14 0.39 0.49 -0.15 -0.13 0.42 0.58 0.42 0.58 ## Question_15 0.28 0.38 -0.25 -0.20 0.32 0.68 0.32 0.68 ## Question_16 0.54 0.28 -0.25 -0.16 0.46 0.54 0.46 0.54 ## Question_17 0.29 0.27 -0.64 -0.05 0.58 0.42 0.58 0.42 ## Question_18 0.37 0.61 -0.14 -0.13 0.54 0.46 0.54 0.46 ## Question_19 -0.28 -0.15 0.06 0.38 0.24 0.76 0.24 0.76 ## Question_20 0.46 0.04 -0.09 -0.20 0.27 0.73 0.27 0.73 ## Question_21 0.59 0.26 -0.15 -0.15 0.47 0.53 0.47 0.53 ## Question_22 -0.03 -0.16 0.07 0.47 0.25 0.75 0.25 0.75 ## Question_23 0.03 -0.04 0.07 0.33 0.12 0.88 0.12 0.88 ## ## PA1 PA3 PA4 PA2 ## SS loadings 3.03 2.85 1.99 1.44 ## Proportion Var 0.13 0.12 0.09 0.06 ## Cumulative Var 0.13 0.26 0.34 0.40 ## Proportion Explained 0.33 0.31 0.21 0.15 ## Cumulative Proportion 0.33 0.63 0.85 1.00 ## ## Standardized loadings (pattern matrix) ## item PA1 PA3 PA4 PA2 h2 u2 ## Question_01 1 0.50 0.22 -0.27 0.00 0.37 0.63 ## Question_02 2 -0.21 -0.03 -0.01 0.46 0.26 0.74 ## Question_03 3 -0.50 -0.18 0.16 0.40 0.47 0.53 ## Question_04 4 0.53 0.28 -0.25 -0.03 0.42 0.58 ## Question_05 5 0.44 0.27 -0.19 -0.05 0.30 0.70 ## Question_06 6 0.05 0.75 -0.12 -0.10 0.59 0.41 ## Question_07 7 0.36 0.56 -0.16 -0.13 0.49 0.51 ## Question_08 8 0.22 0.15 -0.76 0.00 0.65 0.35 ## Question_09 9 -0.13 -0.07 -0.06 0.56 0.34 0.66 ## Question_10 10 0.14 0.38 -0.14 -0.12 0.20 0.80 ## Question_11 11 0.24 0.27 -0.69 -0.17 0.63 0.37 ## Question_12 12 0.51 0.40 -0.11 -0.15 0.45 0.55 ## Question_13 13 0.29 0.56 -0.23 -0.14 0.47 0.53 ## Question_14 14 0.39 0.48 -0.15 -0.13 0.42 0.58 ## Question_15 15 0.28 0.38 -0.25 -0.20 0.32 0.68 ## Question_16 16 0.54 0.28 -0.25 -0.16 0.46 0.54 ## Question_17 17 0.30 0.27 -0.64 -0.05 0.58 0.42 ## Question_18 18 0.37 0.61 -0.14 -0.13 0.54 0.46 ## Question_19 19 -0.28 -0.15 0.06 0.37 0.24 0.76 ## Question_20 20 0.47 0.04 -0.09 -0.20 0.27 0.73 ## Question_21 21 0.60 0.26 -0.15 -0.15 0.47 0.53 ## Question_22 22 -0.03 -0.16 0.07 0.47 0.25 0.75 ## Question_23 23 0.03 -0.04 0.07 0.33 0.12 0.88 ## ## PA1 PA3 PA4 PA2 ## SS loadings 3.03 2.85 1.99 1.44 ## Proportion Var 0.13 0.12 0.09 0.06 ## Cumulative Var 0.13 0.26 0.34 0.40 ## Cum. factor Var 0.33 0.63 0.85 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 4 factors are sufficient. ## ## The degrees of freedom for the null model are 253 and the objective function was 7.55 with Chi Square of 19334.49 ## The degrees of freedom for the model are 167 and the objective function was 0.46 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.03 ## ## The harmonic number of observations is 2571 with the empirical chi square 880.48 with prob &lt; 2.3e-97 ## The total number of observations was 2571 with Likelihood Chi Square = 1166.49 with prob &lt; 2.1e-149 ## ## Tucker Lewis Index of factoring reliability = 0.921 ## RMSEA index = 0.048 and the 90 % confidence intervals are 0.046 0.051 ## BIC = -144.8 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## PA1 PA3 PA4 PA2 ## Correlation of (regression) scores with factors 0.83 0.86 0.86 0.77 ## Multiple R square of scores with factors 0.69 0.73 0.74 0.59 ## Minimum correlation of possible factor scores 0.37 0.46 0.49 0.19 8.2.7 Remarks The method used was “Principal Axis Factoring” and the rotation used was “Varimax”. All differences in results between the software are due to rounding. 8.2.8 References Field, A. (2018). Discovering statistics using IBM SPSS statistics. Los Angeles, CA: SAGE. "],["reliability-module.html", "Chapter 9 Reliability Module 9.1 Classical Single-Test Reliability Analysis", " Chapter 9 Reliability Module 9.1 Classical Single-Test Reliability Analysis An example from Field (2018 pp. 795-796): “I have noticed that a lot of students become very stressed about SPSS Statistics. Imagine that I wanted to design a questionnaire to measure a trait that I termed ‘SPSS anxiety’. I devised a questionnaire to measure various aspects of students’ anxiety towards learning SPSS, the SAQ. I generated questions based on interviews with anxious and non-anxious students and came up with 23 possible questions to include. Each question was a statement followed by a five-point Likert scale: ‘strongly disagree’, ‘disagree’, ‘neither agree nor disagree’, ‘agree’ and ‘strongly agree’ (SD, D, N, A and SA, respectively). What’s more, I wanted to know whether anxiety about SPSS could be broken down into specific forms of anxiety. In other words, what latent variables contribute to anxiety about SPSS? With a little help from a few lecturer friends I collected 2571 completed questionnaires.” Note: Only questions 1, 4, 5, 6, 7, 8, and 10 have been used to simplify the analysis. 9.1.1 Results Overview Table 9.1: Result Overview Reliability Analysis JASP SPSS SAS Minitab R Cronbachs \\(\\alpha\\) 0.7574 0.758 0.7574 0.7574 0.7574 9.1.2 JASP Figure 9.1: JASP Output for Reliability Analysis 9.1.3 SPSS DATASET ACTIVATE DataSet1. RELIABILITY /VARIABLES=Question_01 Question_04 Question_05 Question_06 Question_07 Question_08 Question_10 /SCALE(&#39;ALL VARIABLES&#39;) ALL /MODEL=ALPHA /STATISTICS=DESCRIPTIVE SCALE /SUMMARY=TOTAL MEANS. Figure 9.2: SPSS Output for Reliability Analysis Figure 9.3: SPSS Output for Reliability Analysis 9.1.4 SAS PROC CORR DATA=work.Reliability ALPHA; VAR Q1 Q4 Q5 Q6 Q7 Q8 Q10; RUN; Figure 9.4: SAS Output for Reliability Analysis 9.1.5 Minitab Figure 9.5: Minitab Output for Reliability Analysis Figure 9.6: Minitab Output for Reliability Analysis 9.1.6 R # library(psych) QuestionSelection &lt;- reli.data[c(1,4,5,6,7,8,10)] analysis &lt;- psych::alpha(QuestionSelection, cumulative = T) analysis$total ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd ## 0.7573813 0.757503 0.7440687 0.3085574 3.123762 0.007201353 17.55115 4.311329 ## median_r ## 0.297497 analysis$item.stats ## n raw.r std.r r.cor r.drop mean sd ## Question_01 2571 0.6228945 0.6501973 0.5677797 0.4824078 2.374173 0.8280221 ## Question_04 2571 0.6848824 0.6913312 0.6268423 0.5378476 2.786075 0.9485482 ## Question_05 2571 0.6498827 0.6551175 0.5691098 0.4890456 2.722287 0.9646904 ## Question_06 2571 0.6626598 0.6292060 0.5488700 0.4733243 2.227149 1.1220023 ## Question_07 2571 0.7317687 0.7046681 0.6536507 0.5726486 2.923765 1.1023600 ## Question_08 2571 0.5692230 0.5881757 0.4711300 0.4074534 2.236873 0.8725704 ## Question_10 2571 0.5382484 0.5488999 0.4163395 0.3691902 2.280825 0.8771293 analysis$alpha.drop ## raw_alpha std.alpha G6(smc) average_r S/N alpha se ## Question_01 0.7277017 0.7247454 0.7028417 0.3049925 2.633000 0.008153868 ## Question_04 0.7142260 0.7129289 0.6916327 0.2927412 2.483457 0.008553717 ## Question_05 0.7246553 0.7233622 0.7046391 0.3035270 2.614836 0.008232559 ## Question_06 0.7300262 0.7305573 0.7020402 0.3112445 2.711364 0.008128863 ## Question_07 0.7044218 0.7089704 0.6818863 0.2887689 2.436077 0.008995173 ## Question_08 0.7412689 0.7415166 0.7232435 0.3234650 2.868721 0.007791848 ## Question_10 0.7484933 0.7515384 0.7318020 0.3351629 3.024767 0.007620190 ## var.r med.r ## Question_01 0.007856087 0.2837230 ## Question_04 0.007391559 0.2837230 ## Question_05 0.009125797 0.2974970 ## Question_06 0.006534879 0.3053651 ## Question_07 0.006679362 0.2686270 ## Question_08 0.008291751 0.3053651 ## Question_10 0.007077595 0.3307376 analysis$total$raw_alpha ## [1] 0.7573813 9.1.7 Remarks All differences in results between the software are due to rounding. For this analysis JASP 0.15 was used. 9.1.8 References Field, A. (2018). Discovering statistics using IBM SPSS statistics. Los Angeles, CA: SAGE. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
